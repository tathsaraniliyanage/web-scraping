web-scraping steps

1. import required libs
2. url of the page of scrap 
3. send the get request to the server
4. print the scrap data



csv file

Comma-separated values (CSV) is a text file format that uses commas to separate values, 
and newlines to separate records. A CSV file stores tabular data (numbers and text) in 
plain text, where each line of the file typically represents one data record.




How Web Scraping Works:


Access the Website: You first send a request to a webpage using tools like Python's 
requests or urllib library to download the HTML content.

Parse the HTML: Once the HTML is retrieved, libraries like BeautifulSoup or lxml are 
used to parse the HTML content and extract specific data, such as titles, prices, or links.

Extract and Process Data: After parsing, you can navigate through the HTML structure
(such as finding tags by their class names or IDs) to extract the specific pieces of
data you're interested in.

Store the Data: After extracting the desired information, the data is usually stored in 
a structured format like CSV, JSON, or a database.





Tools and Libraries Used for Web Scraping:


Requests: 
Used to send HTTP requests to fetch web page content.

BeautifulSoup: 
A Python library for parsing HTML and XML documents to extract specific data.

Selenium: 
Used for web scraping dynamic content (web pages that load content with JavaScript).

Scrapy: 
A powerful framework for large-scale web scraping tasks.

Pandas: 
For organizing and storing the extracted data into structured formats like CSV or DataFrames.



